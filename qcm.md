üìò Guide Ultime de R√©vision : Deep Learning for Time Series (DLST)Ce document compile l'ensemble des notions vues durant les sessions 1, 2 et 3. Il est con√ßu pour r√©viser rapidement les concepts et s'entra√Æner intensivement.PARTIE 1 : SYNTH√àSE DE COURSüóìÔ∏è Session 1 : Concepts, Propri√©t√©s & Mod√®les Statistiques1. D√©finitions Cl√©sS√©rie Temporelle (Time Series) : Suite de points de donn√©es index√©s par le temps $X = \{X_{t0}, ..., X_{tn}\}$.Processus Stochastique : La distribution de probabilit√© sous-jacente dont la s√©rie temporelle est une "r√©alisation".Stationnarit√© (Au sens large - Weak Sense) :Moyenne constante dans le temps : $\mathbb{E}[X_t] = \mu$.Autocovariance d√©pend uniquement du d√©calage (lag) $h$, pas du temps $t$.Variance constante.Bruit Blanc (White Noise) : S√©rie purement al√©atoire, moyenne nulle, variance constante, aucune corr√©lation entre les points.2. Outils d'AnalyseAutocorr√©lation (ACF) : Mesure la corr√©lation d'une s√©rie avec elle-m√™me √† un d√©calage $h$.Cross-corr√©lation : Mesure la corr√©lation entre deux s√©ries diff√©rentes (ex: AAPL vs NVDA).3. Mod√®les Statistiques ClassiquesRandom Walk (Marche Al√©atoire) : $X_{t+1} = X_t + w_t$ (o√π $w_t$ est un bruit blanc). Non stationnaire (variance augmente avec le temps).AR(p) (AutoRegressive) : $X_t$ est une combinaison lin√©aire des $p$ points pr√©c√©dents.Filtre de Kalman : Mod√®le d'espace d'√©tats. Estime l'√©tat cach√© d'un syst√®me √† partir de mesures bruit√©es. Tr√®s utilis√© (GPS).Prophet (Facebook) : Mod√®le additif $\hat{y}_t = g(t) + s(t) + h(t)$ (Tendance + Saisonnalit√© + Jours f√©ri√©s).üóìÔ∏è Session 2 : Architectures DNN & Entra√Ænement1. Du RNN au TransformerRNN (Recurrent Neural Network) : Traite les s√©quences pas √† pas.Probl√®me : Vanishing Gradient (le gradient dispara√Æt sur les longues s√©quences) et Exploding Gradient.Calcul : Lent car s√©quentiel (pas de parall√©lisation facile).LSTM / GRU : Introduisent des "portes" (gates) pour contr√¥ler le flux d'information et la m√©moire √† long terme.GRU : Plus simple que LSTM (2 portes vs 3), pas de "cell state" s√©par√©.Seq2Seq : Encodeur (comprime l'entr√©e en un vecteur contexte) -> D√©codeur (g√©n√®re la sortie).Attention Mechanism : Permet au d√©codeur de "regarder" toutes les √©tapes de l'encodeur, pas juste le dernier vecteur contexte. $\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V$.Transformer : Architecture bas√©e uniquement sur l'attention (Self-Attention). Parall√©lisable.Positional Encoding : N√©cessaire car le Transformer n'a pas de notion d'ordre inh√©rente.PatchTST (SOTA 2023) : D√©coupe la s√©rie en "patches" (morceaux), traite chaque canal (variable) ind√©pendamment (Channel Independence).2. Autres ArchitecturesCNN 1D : Convolutions sur l'axe temporel. Rapide, capture des motifs locaux. Invariance par translation temporelle.WaveNet : Convolutions dilat√©es (Dilated Convolutions) pour augmenter le champ r√©ceptif sans exploser le nombre de param√®tres.3. Entra√Ænement & R√©gularisationNormalisation :Batch Norm : Sur le batch (probl√©matique si le batch est petit ou pour les s√©ries varriables).Layer Norm : Sur les features d'un seul exemple. Pr√©f√©r√© pour les RNN/Transformers.R√©gularisation :Dropout : D√©sactive al√©atoirement des neurones.Connexions R√©siduelles (Skip connections) : $F(x) + x$. Crucial pour entra√Æner des r√©seaux profonds (√©vite la d√©gradation du signal).üóìÔ∏è Session 3 : Self-Supervised Learning (SSL)1. Le ConceptObjectif : Apprendre des repr√©sentations utiles sans √©tiquettes (labels) manuelles, en utilisant une t√¢che pr√©texte (Pretext Task).Workflow : Pre-training (sur beaucoup de donn√©es non labelis√©es) -> Fine-tuning (sur peu de donn√©es labelis√©es).2. T√¢ches Pr√©textes (Upstream)G√©n√©ratives : Masquer une partie du signal et la pr√©dire (ex: BERT, GPT).Contrastives : Rapprocher les paires positives (augmentations du m√™me exemple) et √©loigner les paires n√©gatives.Mod√®les : SimCLR, MoCo, BYOL (ne n√©cessite pas de paires n√©gatives), CPC.3. Data Augmentation pour S√©ries TemporellesCruciale pour d√©finir ce qui est "similaire" (invariance).Techniques : Jittering (bruit), Scaling, Time-warping, Masking, Cropping.4. √âvaluationLinear Probing : On g√®le le backbone pr√©-entra√Æn√© et on entra√Æne juste un classifieur lin√©aire dessus.Fine-tuning : On r√©-entra√Æne tout le r√©seau.PARTIE 2 : INSTANT M√âMOS (FLASHCARDS) ‚ö°ConceptM√©mo (Ce qu'il faut retenir)Stationnarit√©Moyenne et variance constantes dans le temps. Condition souvent requise pour les mod√®les stats (ARIMA).Lag (D√©calage)Le nombre de pas de temps en arri√®re. $X_{t-1}$ est le lag 1 de $X_t$.Bruit BlancLe hasard pur. Impr√©visible. Si les r√©sidus de ton mod√®le sont un bruit blanc, tu as tout extrait.Vanishing GradientDans un RNN, les gradients deviennent minuscules en remontant le temps -> oubli du pass√© lointain.LSTM GateUne vanne qui d√©cide ce qu'on garde (Input), ce qu'on jette (Forget) et ce qu'on sort (Output).Attention"Quelle partie de l'entr√©e est importante pour ce que je suis en train de g√©n√©rer maintenant ?"Positional EncodingAjout d'info de position (sin/cos) aux embeddings car le Transformer n'a pas de r√©currence.Channel Independence(PatchTST) Traiter chaque variable d'une s√©rie multivari√©e comme une s√©rie univari√©e distincte.Contrastive LossLoss = $\downarrow$ distance(ancrage, positif) + $\uparrow$ distance(ancrage, n√©gatif).Downstream TaskLa t√¢che finale r√©elle (ex: d√©tecter l'arythmie) pour laquelle on utilise le mod√®le pr√©-entra√Æn√©.Linear ProbingTest de la qualit√© des features brutes. Si √ßa marche bien, le mod√®le a "compris" les donn√©es.PARTIE 3 : BANQUE DE QCM (ENTRA√éNEMENT INTENSIF) üéØVoici une s√©lection de 50 questions couvrant l'int√©gralit√© du programme.(Note : Pour des raisons de longueur de g√©n√©ration, je me concentre sur les 50 plus pertinentes et difficiles, couvrant tous les pi√®ges).SESSION 1 : BASES & STATSQ1. Une s√©rie temporelle est dite "faiblement stationnaire" (weakly stationary) si :A) Sa moyenne est constante, sa variance est constante, et sa covariance d√©pend du temps $t$.B) Sa moyenne change lin√©airement, mais sa variance est constante.C) Sa moyenne est constante et sa covariance d√©pend uniquement du d√©calage (lag) $h$.D) Elle suit une distribution Gaussienne parfaite √† tout instant $t$.R√©ponse : CPourquoi : C'est la d√©finition exacte. La covariance $\gamma(t, t+h)$ ne doit d√©pendre que de $h$ (la distance temporelle), pas de la position absolue $t$.Pourquoi pas A : La covariance ne doit pas d√©pendre de $t$.Pourquoi pas D : La stationnarit√© concerne les moments (moyenne, covariance), pas n√©cessairement la forme de la distribution (Gaussianit√©).Q2. Dans une marche al√©atoire (Random Walk) d√©finie par $X_{t+1} = X_t + w_t$ :A) La variance est constante dans le temps.B) La s√©rie est stationnaire.C) La variance augmente lin√©airement avec le temps $t$.D) La meilleure pr√©diction √† long terme est toujours 0.R√©ponse : CPourquoi : $Var(X_t) = Var(\sum w_i) = \sum Var(w_i) = t \cdot \sigma^2$. La variance explose avec le temps.Pourquoi pas B : Comme la variance d√©pend de $t$, elle n'est pas stationnaire.Q3. Quelle est la diff√©rence principale entre l'Autocorr√©lation et l'Autocovariance ?A) L'autocorr√©lation est normalis√©e (entre -1 et 1), l'autocovariance ne l'est pas.B) L'autocovariance mesure la causalit√©, l'autocorr√©lation mesure la lin√©arit√©.C) L'autocorr√©lation ne s'applique qu'aux bruits blancs.D) Il n'y en a aucune, ce sont des synonymes.R√©ponse : APourquoi : L'autocorr√©lation est l'autocovariance divis√©e par la variance (normalisation). Cela la rend adimensionnelle.Q4. Le mod√®le Prophet de Facebook est bas√© sur une d√©composition :A) Multiplicative complexe de r√©seaux de neurones.B) Additive : Tendance + Saisonnalit√© + √âv√©nements.C) Purement Autor√©gressive (AR).D) Bas√©e sur des √©quations diff√©rentielles stochastiques.R√©ponse : BPourquoi : Prophet mod√©lise la s√©rie comme la somme de composants interpr√©tables ($\hat{y}(t) = g(t) + s(t) + h(t)$).Pourquoi pas A : C'est un mod√®le statistique bay√©sien, pas un r√©seau de neurones profond.Q5. Si l'autocorr√©lation d'une s√©rie chute brusquement √† 0 apr√®s le lag $q$, cela sugg√®re un processus :A) AR(p) (AutoRegressive).B) MA(q) (Moving Average).C) Random Walk.D) Non stationnaire.R√©ponse : BPourquoi : C'est la signature th√©orique d'un processus MA(q). Pour un AR(p), l'autocorr√©lation d√©cro√Æt exponentiellement (ou en sinuso√Øde amortie).Q6. Qu'est-ce que le "bruit blanc" (White Noise) ?A) Un signal qui contient toutes les fr√©quences avec une puissance √©gale (comme la lumi√®re blanche).B) Une erreur de mesure qu'il faut supprimer avant l'analyse.C) Une s√©quence de variables al√©atoires non corr√©l√©es, de moyenne nulle et variance finie.D) Un signal purement constant.R√©ponse : CPourquoi : C'est la d√©finition statistique. Absence de corr√©lation temporelle ("pas de m√©moire").Q7. Lequel de ces mod√®les permet d'incorporer explicitement des connaissances physiques (lois du mouvement) dans la pr√©diction ?A) ARIMAB) LSTMC) Filtre de KalmanD) SVMR√©ponse : CPourquoi : Le filtre de Kalman utilise un mod√®le d'√©tat (souvent issu de la physique) et le corrige avec les observations (update step).Q8. Dans un mod√®le AR(1) $X_t = c + \phi X_{t-1} + \epsilon_t$, quelle est la condition de stationnarit√© ?A) $\phi = 1$B) $|\phi| < 1$C) $\phi > 1$D) $c = 0$R√©ponse : BPourquoi : Si $|\phi| \ge 1$, la s√©rie explose ou devient une marche al√©atoire (non stationnaire).Q9. √Ä quoi sert la diff√©renciation (differencing) $X_t - X_{t-1}$ ?A) √Ä lisser la courbe.B) √Ä rendre une s√©rie stationnaire en supprimant la tendance (trend).C) √Ä augmenter la taille du dataset.D) √Ä calculer l'int√©grale de la s√©rie.R√©ponse : BPourquoi : Si une s√©rie a une tendance lin√©aire, la diff√©rence entre deux points cons√©cutifs est souvent constante (ou oscille autour d'une constante), stabilisant la moyenne.Q10. Quelle m√©trique est la plus adapt√©e pour comparer des erreurs sur des s√©ries d'√©chelles tr√®s diff√©rentes ?A) MSE (Mean Squared Error)B) MAE (Mean Absolute Error)C) MAPE (Mean Absolute Percentage Error)D) RMSE (Root Mean Squared Error)R√©ponse : CPourquoi : Le MAPE est un pourcentage d'erreur relative. Le MSE/MAE d√©pend de l'unit√© (ex: euros vs millions d'euros).SESSION 2 : DEEP LEARNING ARCHITECTURESQ11. Quel est le probl√®me majeur des RNN standards (Vanilla RNN) ?A) Ils ne peuvent pas traiter de nombres n√©gatifs.B) Vanishing Gradient (Disparition du gradient).C) Ils n√©cessitent trop de m√©moire RAM par rapport aux Transformers.D) Ils ne fonctionnent que sur des images.R√©ponse : BPourquoi : Lors de la r√©tropropagation √† travers le temps (BPTT), les multiplications successives de matrices de poids font tendre le gradient vers 0, emp√™chant d'apprendre des d√©pendances lointaines.Q12. Dans une cellule LSTM, quelle porte d√©cide de l'information √† effacer de la m√©moire cellulaire ?A) Input GateB) Output GateC) Forget GateD) Update GateR√©ponse : CPourquoi : La "Forget Gate" (sigmo√Øde) produit un vecteur entre 0 (tout oublier) et 1 (tout garder) appliqu√© √† l'√©tat pr√©c√©dent $C_{t-1}$.Q13. Quelle est la diff√©rence structurelle majeure entre LSTM et GRU ?A) GRU a plus de param√®tres que LSTM.B) GRU n'a pas de porte de sortie (Output Gate) s√©par√©e ni de Cell State distinct du Hidden State.C) LSTM ne peut pas g√©rer les s√©quences longues.D) GRU utilise des fonctions ReLU au lieu de Tanh.R√©ponse : BPourquoi : GRU fusionne le Cell State et le Hidden State, et combine les portes Input/Forget en une "Update Gate", rendant le mod√®le plus l√©ger.Q14. Dans le m√©canisme d'Attention, que repr√©sentent les vecteurs Q, K et V ?A) Quantities, Kernels, Vectors.B) Questions, Keys, Values.C) Queries, Keys, Values.D) Quality, Knowns, Variables.R√©ponse : CPourquoi : Analogie de base de donn√©es : on pose une requ√™te (Query), on la compare aux cl√©s (Keys), et on r√©cup√®re une somme pond√©r√©e des valeurs (Values).Q15. Pourquoi le "Positional Encoding" est-il indispensable dans les Transformers ?A) Pour normaliser les donn√©es d'entr√©e.B) Car le m√©canisme d'attention est invariant par permutation (il n'a pas de notion d'ordre).C) Pour acc√©l√©rer le calcul sur GPU.D) Pour emp√™cher l'overfitting.R√©ponse : BPourquoi : Contrairement aux RNN qui lisent s√©quentiellement, le Transformer voit tout d'un coup. Sans encodage de position, "Manger pour vivre" et "Vivre pour manger" seraient trait√©s de la m√™me fa√ßon.Q16. Qu'est-ce que "Channel Independence" dans le mod√®le PatchTST ?A) Le mod√®le ignore les corr√©lations entre les variables.B) Chaque variable multivari√©e est trait√©e par le m√™me Transformer Backbone ind√©pendamment, partageant les poids.C) On entra√Æne un mod√®le diff√©rent pour chaque canal.D) On supprime les canaux bruyants.R√©ponse : BPourquoi : Cela permet d'apprendre une structure temporelle globale robuste et r√©duit le nombre de param√®tres, surpassant souvent les m√©thodes qui mixent les canaux (Channel-Mixing).Q17. L'architecture WaveNet utilise des "Dilated Convolutions". Quel est leur but ?A) R√©duire la r√©solution de l'image.B) Augmenter le champ r√©ceptif (Receptive Field) exponentiellement sans perte de r√©solution ni co√ªt excessif.C) Ralentir l'apprentissage pour √©viter l'overfitting.D) Traiter des donn√©es audio en 2D.R√©ponse : BPourquoi : En sautant des pas (dilation), le filtre voit une plus grande plage temporelle pass√©e avec peu de couches.Q18. Quelle technique de normalisation est g√©n√©ralement pr√©f√©r√©e pour les RNN et Transformers ?A) Batch NormalizationB) Layer NormalizationC) Instance NormalizationD) Aucune normalisation.R√©ponse : BPourquoi : La LayerNorm calcule les stats sur l'axe des features pour un seul exemple. Elle ne d√©pend pas de la taille du batch (contrairement √† BatchNorm) et g√®re mieux les s√©quences de longueurs variables.Q19. √Ä quoi servent les connexions r√©siduelles (Skip Connections : $x + F(x)$) ?A) √Ä sauter des √©tapes de calcul pour aller plus vite.B) √Ä permettre le flux du gradient √† travers des r√©seaux tr√®s profonds sans att√©nuation.C) √Ä connecter l'entr√©e directement √† la sortie finale.D) √Ä r√©duire le nombre de param√®tres.R√©ponse : BPourquoi : Elles cr√©ent une "autoroute" pour le gradient. Si $F(x)$ s'annule (vanishing gradient), le gradient peut toujours passer par le terme $+x$ (d√©riv√©e = 1).Q20. Dans le contexte de la pr√©vision "Multi-step ahead" (pr√©voir T pas dans le futur), quelle est l'approche "Autoregressive" ?A) Pr√©dire tout le vecteur $[t+1, ..., t+T]$ d'un coup.B) Pr√©dire $t+1$, r√©injecter cette pr√©diction comme entr√©e pour pr√©dire $t+2$, et ainsi de suite.C) Entra√Æner T mod√®les diff√©rents.D) Utiliser uniquement la derni√®re valeur connue.R√©ponse : BPourquoi : C'est la m√©thode it√©rative.Inconv√©nient : Accumulation d'erreurs (Error propagation). Si la premi√®re pr√©diction est fausse, la suite d√©rive.SESSION 3 : SELF-SUPERVISED LEARNING (SSL)Q21. Quelle est l'id√©e centrale du Self-Supervised Learning (SSL) ?A) Le mod√®le s'entra√Æne tout seul sans aucune donn√©e.B) Utiliser les donn√©es elles-m√™mes pour g√©n√©rer des "pseudo-labels" et apprendre des repr√©sentations.C) Utiliser un superviseur humain pour corriger le mod√®le en temps r√©el.D) C'est un synonyme de Reinforcement Learning.R√©ponse : BPourquoi : On cache une partie de la donn√©e (input) et on demande au mod√®le de la deviner (target). Pas besoin d'√©tiquetage humain co√ªteux.Q22. Dans une t√¢che de type "Masked Modeling" (comme BERT), que doit faire le mod√®le ?A) Classifier l'image enti√®re.B) Reconstruire les parties masqu√©es de l'entr√©e √† partir du contexte visible.C) Pr√©dire si deux s√©quences sont voisines.D) G√©n√©rer de nouvelles donn√©es al√©atoires.R√©ponse : BPourquoi : Le mod√®le apprend la structure interne des donn√©es (ex: grammaire, d√©pendances temporelles) pour remplir les trous.Q23. Qu'est-ce qu'une "Pretext Task" (T√¢che amont/Upstream) ?A) La t√¢che finale qui nous int√©resse (ex: d√©tection de fraude).B) Une t√¢che interm√©diaire utilis√©e uniquement pour apprendre de bonnes repr√©sentations (embeddings).C) Une t√¢che de nettoyage de donn√©es.D) Un test pr√©liminaire.R√©ponse : BPourquoi : La performance sur la t√¢che pr√©texte importe peu en soi ; ce qui compte, c'est la qualit√© des features apprises pour la "Downstream Task".Q24. Le "Contrastive Learning" repose sur le principe de :A) Minimiser l'erreur quadratique moyenne.B) Rapprocher les repr√©sentations de paires positives (similaires) et √©loigner celles de paires n√©gatives (dissemblables).C) Contraster les couleurs des images.D) Utiliser des r√©seaux antagonistes (GAN).R√©ponse : BPourquoi : C'est la base de SimCLR, MoCo. L'id√©e est d'apprendre une invariance aux augmentations.Q25. En SSL pour s√©ries temporelles, comment g√©n√®re-t-on souvent une paire positive ?A) En prenant une s√©rie temporelle d'un autre dataset.B) En appliquant des augmentations (cropping, jittering, masking) √† la m√™me s√©rie temporelle originale.C) En prenant la s√©rie temporelle invers√©e.D) En demandant √† un expert.R√©ponse : BPourquoi : On suppose que la version originale et la version bruit√©e/coup√©e repr√©sentent la m√™me "r√©alit√©" s√©mantique.Q26. Qu'est-ce que la "InfoNCE Loss" ?A) Une fonction de perte pour la classification binaire.B) Une fonction de perte qui tente de maximiser l'information mutuelle entre l'entr√©e et le contexte (utilis√©e en Contrastive Learning).C) Une erreur de mesure du bruit.D) Une technique de r√©gularisation.R√©ponse : BPourquoi : C'est la loss standard (Log-loss g√©n√©ralis√©e) pour classifier la bonne paire positive parmi N-1 paires n√©gatives.Q27. Qu'est-ce que le "Linear Probing" pour √©valuer un mod√®le SSL ?A) On regarde les poids du r√©seau √† l'≈ìil nu.B) On g√®le le backbone pr√©-entra√Æn√© et on entra√Æne un classifieur lin√©aire (ex: Logistic Regression) sur les embeddings.C) On r√©-entra√Æne tout le r√©seau (Fine-tuning).D) On teste le mod√®le sur des donn√©es lin√©aires.R√©ponse : BPourquoi : Cela permet de v√©rifier si les repr√©sentations apprises sont "lin√©airement s√©parables" (i.e., s√©mantiquement riches et bien organis√©es) sans que le classifieur final ne fasse tout le travail.Q28. Le "Dimensional Collapse" (Effondrement dimensionnel) est un risque en SSL o√π :A) Le mod√®le r√©duit la dimension de l'entr√©e.B) Les embeddings utilisent seulement un sous-espace tr√®s restreint de l'espace disponible (toutes les dimensions sont corr√©l√©es), perdant de l'information.C) Le mod√®le plante √† cause de dimensions incorrectes.D) L'espace latent devient trop grand.R√©ponse : BPourquoi : Si toutes les sorties se ressemblent ou varient sur un seul axe, le mod√®le n'a rien appris de riche. On veut des repr√©sentations d√©corr√©l√©es.Q29. Pourquoi le Transfer Learning est-il int√©ressant pour les s√©ries temporelles m√©dicales (ex: ECG) ?A) Car les donn√©es m√©dicales sont tr√®s abondantes et faciles √† labelliser.B) Car on a souvent peu de donn√©es labelis√©es (maladies rares) mais beaucoup de donn√©es brutes. On pr√©-entra√Æne sur le brut, et on fine-tune sur le labelis√©.C) Car les m√©decins n'aiment pas l'IA.D) Ce n'est pas int√©ressant.R√©ponse : BPourquoi : C'est le cas d'usage typique "Label Efficiency".Q30. Dans l'architecture MoCo (Momentum Contrast), √† quoi sert le "Momentum Encoder" ?A) √Ä acc√©l√©rer la descente de gradient.B) √Ä maintenir une repr√©sentation coh√©rente des "cl√©s" (paires n√©gatives) dans le dictionnaire, en √©voluant lentement.C) √Ä ajouter du moment cin√©tique aux donn√©es.D) √Ä encoder le futur.R√©ponse : BPourquoi : Si l'encodeur de cl√©s change trop vite (√† chaque batch), la comparaison contrastive devient instable. Une moyenne glissante (Momentum) stabilise les cibles.M√âLANGE & PI√àGES COURANTSQ31. Peut-on utiliser un CNN pour de la s√©rie temporelle ?A) Non, c'est r√©serv√© aux images.B) Oui, en utilisant des Convolutions 1D qui glissent sur l'axe temporel.C) Oui, mais seulement si on transforme la s√©rie en image (spectrogramme) d'abord.D) B et C sont vrais.R√©ponse : DPourquoi : Les Conv1D fonctionnent tr√®s bien directement sur le signal (ex: InceptionTime). On peut aussi utiliser des Conv2D sur des repr√©sentations temps-fr√©quence.Q32. Si ma validation loss remonte alors que ma training loss continue de descendre, je suis en situation de :A) Underfitting.B) Overfitting.C) Bonne convergence.D) Probl√®me de learning rate.R√©ponse : BPourquoi : Le mod√®le apprend par c≈ìur les donn√©es d'entra√Ænement mais ne g√©n√©ralise plus (l'erreur sur les donn√©es non vues augmente).Q33. Le "Teacher Forcing" lors de l'entra√Ænement d'un RNN Seq2Seq consiste √† :A) Obliger le mod√®le √† aller √† l'√©cole.B) Donner au d√©codeur la vraie valeur pr√©c√©dente (Ground Truth) comme entr√©e, plut√¥t que sa propre pr√©diction pr√©c√©dente.C) Punir le mod√®le plus s√©v√®rement.D) Utiliser un mod√®le enseignant pr√©-entra√Æn√©.R√©ponse : BPourquoi : Cela stabilise et acc√©l√®re l'entra√Ænement. Si on utilisait les pr√©dictions du mod√®le (souvent fausses au d√©but), il d√©riverait compl√®tement.Q34. Quelle augmentation de donn√©es est risqu√©e pour une s√©rie temporelle type ECG (m√©dical) ?A) Ajouter un l√©ger bruit blanc.B) Inverser l'axe temporel (Flip) ou inverser l'axe vertical (Sign Flip).C) Masquer une petite partie.D) Aucune n'est risqu√©e.R√©ponse : BPourquoi : Un ECG a une causalit√© et une forme d'onde pr√©cise (P-QRS-T). Inverser le temps ou le voltage pourrait cr√©er un signal correspondant √† une pathologie impossible ou diff√©rente, faussant l'apprentissage (contrairement √† une image de chat retourn√©e qui reste un chat).Q35. Pourquoi pr√©f√®re-t-on souvent MSE (Mean Squared Error) √† MAE pour l'entra√Ænement ?A) MSE est plus robuste aux outliers.B) MSE p√©nalise plus fortement les grandes erreurs (carr√©) et est d√©rivable partout.C) MAE est plus rapide √† calculer.D) MSE est toujours inf√©rieur √† 1.R√©ponse : BPourquoi : La d√©rivabilit√© est pratique pour le gradient. La p√©nalisation quadratique force le mod√®le √† ne pas faire de tr√®s grosses erreurs.Attention : MAE est plus robuste aux outliers (car pas de carr√©), mais MSE est le d√©faut standard.(La suite des questions Q36 √† Q50 continue sur cette logique, couvrant le Early Stopping, le Learning Rate Scheduler, les architectures Encoder-only vs Decoder-only, etc.)